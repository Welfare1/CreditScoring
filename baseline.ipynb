{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "571e4885-d488-4b8a-a9a8-67edf8b49d2b",
    "_uuid": "8383fc8c-dcef-4b8a-9949-be6350d98041",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# African Credit Scoring Challenge - Baseline Notebook \n",
    "\n",
    "\n",
    "## <a id=\"1\"></a>Introduction\n",
    "This notebook provides a baseline approach for the African Credit Scoring Challenge. We will:\n",
    "- Load and understand the credit scoring data\n",
    "- Perform exploratory data analysis\n",
    "- Create and evaluate a machine learning model\n",
    "- Generate predictions for submission\n",
    "\n",
    "Key aspects of this challenge:\n",
    "- Predicting loan defaults in African markets\n",
    "- Dealing with imbalanced data\n",
    "- Building models that can generalize to new regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "799e1fbb-7957-43e7-a45e-b4c7abad664e",
    "_uuid": "21160f1e-c64b-4a38-947f-00b4d0f7857b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-28T15:32:53.706256Z",
     "iopub.status.busy": "2025-03-28T15:32:53.705840Z",
     "iopub.status.idle": "2025-03-28T15:32:58.975247Z",
     "shell.execute_reply": "2025-03-28T15:32:58.973912Z",
     "shell.execute_reply.started": "2025-03-28T15:32:53.706218Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Import Libraries ===\n",
    "# Data manipulation and analysis\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Advanced ML Models\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "96df4b22-c49e-475e-8a0e-4e14a0088cb4",
    "_uuid": "b2ac6532-3f73-46e6-ad2e-2dc819dfaae2",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## <a id=\"2\"></a>Data Loading and Inspection\n",
    "Let's load our training and test datasets and examine their basic properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "70c01309-7b06-4702-bd36-c308dfd7a877",
    "_uuid": "7660cb3f-5e3c-40e0-abfd-0f9e05da5a9f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-28T15:32:58.977216Z",
     "iopub.status.busy": "2025-03-28T15:32:58.976502Z",
     "iopub.status.idle": "2025-03-28T15:32:59.507349Z",
     "shell.execute_reply": "2025-03-28T15:32:59.505909Z",
     "shell.execute_reply.started": "2025-03-28T15:32:58.977166Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loading the train dataset\n",
    "train = pd.read_csv('/data/Train.csv')\n",
    "test = pd.read_csv('/data/Test.csv')\n",
    "# Display the first few rows of the datasets and their shape\n",
    "display(\"Train\", train.head(), train.shape, \"Test\", test.head(), test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "207ab411-3b8f-40f4-97e1-b34e49a9ab0b",
    "_uuid": "f5c5d807-2768-40c7-8855-4f66f7ddaf20",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-28T15:32:59.510171Z",
     "iopub.status.busy": "2025-03-28T15:32:59.509760Z",
     "iopub.status.idle": "2025-03-28T15:32:59.563056Z",
     "shell.execute_reply": "2025-03-28T15:32:59.561951Z",
     "shell.execute_reply.started": "2025-03-28T15:32:59.510131Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Examine data types and check for missing values\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f59c5342-f2e5-4024-9183-729637b24811",
    "_uuid": "77199815-7c0b-4b1a-abca-12851ac6d861",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-28T15:32:59.565032Z",
     "iopub.status.busy": "2025-03-28T15:32:59.564683Z",
     "iopub.status.idle": "2025-03-28T15:32:59.594138Z",
     "shell.execute_reply": "2025-03-28T15:32:59.593016Z",
     "shell.execute_reply.started": "2025-03-28T15:32:59.565002Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check for missing values in the train dataset\n",
    "print(f\"There are {train.isna().sum().sum()} missing values in the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1e68f308-bfae-4e08-b9ca-f23d45921c3c",
    "_uuid": "adc312cb-6a3b-4460-b8b1-9002e9db0565",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## <a id=\"4\"></a>Exploratory Data Analysis (EDA)\n",
    "We'll create visualization helper functions to understand our data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bca603d9-3ed1-4672-80eb-80fada1f29c7",
    "_uuid": "da715118-fb8c-48bb-82bd-8d63dc6c3783",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-28T15:32:59.595662Z",
     "iopub.status.busy": "2025-03-28T15:32:59.595223Z",
     "iopub.status.idle": "2025-03-28T15:32:59.607953Z",
     "shell.execute_reply": "2025-03-28T15:32:59.606766Z",
     "shell.execute_reply.started": "2025-03-28T15:32:59.595625Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_continuous(df, column, transform=None):\n",
    "    \"\"\"\n",
    "    Creates visualization for continuous variables with optional transformations.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Input dataframe\n",
    "    column (str): Column name to visualize\n",
    "    transform (str): Type of transformation ('log' or 'sqrt')\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    \n",
    "    # Original data plots\n",
    "    sns.histplot(df[column], kde=True, ax=axs[0, 0])\n",
    "    axs[0, 0].set_title(f'Distribution of {column}')\n",
    "    axs[0, 0].set_xlabel(column)\n",
    "\n",
    "    sns.boxplot(y=df[column], ax=axs[1, 0])\n",
    "    axs[1, 0].set_title(f'Box Plot of {column}')\n",
    "    axs[1, 0].set_ylabel(column)\n",
    "\n",
    "    if transform == 'log':\n",
    "        transformed_data = np.log1p(df[column])\n",
    "        transform_label = 'Log'\n",
    "    elif transform == 'sqrt':\n",
    "        transformed_data = np.sqrt(df[column])\n",
    "        transform_label = 'Square Root'\n",
    "    else:\n",
    "        transformed_data = None\n",
    "\n",
    "    if transformed_data is not None:\n",
    "        sns.histplot(transformed_data, kde=True, ax=axs[0, 1])\n",
    "        axs[0, 1].set_title(f'{transform_label} Distribution of {column}')\n",
    "        axs[0, 1].set_xlabel(f'{transform_label}({column})')\n",
    "\n",
    "        sns.boxplot(y=transformed_data, ax=axs[1, 1])\n",
    "        axs[1, 1].set_title(f'{transform_label} Box Plot of {column}')\n",
    "        axs[1, 1].set_ylabel(f'{transform_label}({column})')\n",
    "    else:\n",
    "        for ax in [axs[0, 1], axs[1, 1]]:\n",
    "            ax.remove()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_categorical(df, column):\n",
    "    \"\"\"\n",
    "    Creates visualization for categorical variables.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Input dataframe\n",
    "    column (str): Column name to visualize\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    value_counts = df[column].value_counts()\n",
    "    sns.barplot(x=value_counts.index, y=value_counts.values, ax=ax1)\n",
    "    ax1.set_title(f'Bar Plot of {column}')\n",
    "    ax1.set_xlabel(column)\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    ax2.pie(value_counts.values, labels=value_counts.index, autopct='%1.1f%%')\n",
    "    ax2.set_title(f'Pie Chart of {column}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "84dd47f7-42eb-4375-afe2-84e05c85b21d",
    "_uuid": "81fb0a52-6756-4f0f-ba12-6c4c300202bf",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Categorical Variables Analysis\n",
    "Let's explore the distribution of categorical variables in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d9214fb6-bd59-49c0-b5b5-77ba3c454191",
    "_uuid": "6266a1a4-9bae-4e6e-bbdf-ff6b84f3f76d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-28T15:32:59.609705Z",
     "iopub.status.busy": "2025-03-28T15:32:59.609269Z",
     "iopub.status.idle": "2025-03-28T15:33:00.080233Z",
     "shell.execute_reply": "2025-03-28T15:33:00.078950Z",
     "shell.execute_reply.started": "2025-03-28T15:32:59.609660Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Analyze country distribution in train set\n",
    "plot_categorical(train, 'country_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "155fa455-10f3-47e0-887c-714b4fff737d",
    "_uuid": "28e9c932-e18d-45bc-8162-a69863801c66",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-28T15:33:00.081790Z",
     "iopub.status.busy": "2025-03-28T15:33:00.081469Z",
     "iopub.status.idle": "2025-03-28T15:33:00.404705Z",
     "shell.execute_reply": "2025-03-28T15:33:00.403666Z",
     "shell.execute_reply.started": "2025-03-28T15:33:00.081761Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Analyze country distribution in test set\n",
    "plot_categorical(test, 'country_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "512e1e84-207f-4b53-acbe-28ba77aa7be0",
    "_uuid": "8b4ad3b7-d85a-4f4b-8650-528631afcbc5",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "**Important Note:** The test set includes Ghana, which is not present in the training data. This indicates we need to build a model that can generalize well to new regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eafa83a4-af72-425e-8eb0-5dded8afe742",
    "_uuid": "a8f87dd8-6ebf-416c-801d-7897f5ee1904",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-28T15:33:00.408539Z",
     "iopub.status.busy": "2025-03-28T15:33:00.408204Z",
     "iopub.status.idle": "2025-03-28T15:33:00.979034Z",
     "shell.execute_reply": "2025-03-28T15:33:00.977938Z",
     "shell.execute_reply.started": "2025-03-28T15:33:00.408512Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Analyze lender distribution\n",
    "plot_categorical(train, 'lender_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d36b473d-951c-4663-a8ca-8702d8d57409",
    "_uuid": "5fb07d30-f0ad-41dc-aafe-19e7fe9042cb",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Target Variable Analysis\n",
    "Let's examine our target variable distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "643f83e3-d620-41cf-b1e7-4c6bf01c5759",
    "_uuid": "3db3cf92-4378-45d8-981f-8d06cb8cef7e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-28T15:33:00.981321Z",
     "iopub.status.busy": "2025-03-28T15:33:00.980948Z",
     "iopub.status.idle": "2025-03-28T15:33:01.416525Z",
     "shell.execute_reply": "2025-03-28T15:33:01.415402Z",
     "shell.execute_reply.started": "2025-03-28T15:33:00.981290Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Analyze target variable distribution\n",
    "plot_categorical(train, 'target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4069ef60-596f-4165-857c-41f1444338eb",
    "_uuid": "f3061d61-eec0-42e2-ac16-ad15b22dcef7",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "**Key Insight:** The dataset is highly imbalanced, which is typical for loan default scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1f53f227-7f20-4196-8432-51a7de69a9ec",
    "_uuid": "dcf525d9-1623-49aa-afe3-afa90e9afb9d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Continuous Variables Analysis\n",
    "Let's examine the distribution of numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b9efd751-7c2b-4372-a217-e2f7864ef94d",
    "_uuid": "d69b2e62-77e3-4dbe-9c37-a75398e8315a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-28T15:33:01.418235Z",
     "iopub.status.busy": "2025-03-28T15:33:01.417774Z",
     "iopub.status.idle": "2025-03-28T15:34:13.192204Z",
     "shell.execute_reply": "2025-03-28T15:34:13.191030Z",
     "shell.execute_reply.started": "2025-03-28T15:33:01.418201Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Analyze Total Amount distribution\n",
    "plot_continuous(train, 'Total_Amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7c343dfc-ab14-43ec-81d3-a0cebe79d769",
    "_uuid": "6baf8850-76a7-4287-ae9b-b515642951b5",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## <a id=\"5\"></a>Feature Engineering\n",
    "We'll combine train and test data for consistent feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fbdb84c5-2fd6-4367-932b-d7702a1c0ef0",
    "_uuid": "0f273e73-435e-4b7e-aadc-53f541afcf92",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-28T15:34:13.193639Z",
     "iopub.status.busy": "2025-03-28T15:34:13.193341Z",
     "iopub.status.idle": "2025-03-28T15:34:13.604029Z",
     "shell.execute_reply": "2025-03-28T15:34:13.602984Z",
     "shell.execute_reply.started": "2025-03-28T15:34:13.193614Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Combine datasets for consistent feature engineering\n",
    "data = pd.concat([train, test]).reset_index(drop=True)\n",
    "\n",
    "# Convert date columns to datetime\n",
    "data['disbursement_date'] = pd.to_datetime(data['disbursement_date'], errors='coerce')\n",
    "data['due_date'] = pd.to_datetime(data['due_date'], errors='coerce')\n",
    "\n",
    "# update total amount to repay to the the lender portion to be repaid where total amount to be repaid is 0\n",
    "data.loc[data['Total_Amount_to_Repay'] == 0, ['Total_Amount_to_Repay']] += data['Lender_portion_to_be_repaid']\n",
    "\n",
    "# Compute mena and median values by customer for the total amount to repay\n",
    "aggregates = data.groupby('customer_id')['Total_Amount_to_Repay'].agg(['mean', 'median']).reset_index()\n",
    "aggregates.rename(columns={'mean': 'Mean_Total_Amount', 'median': 'Median_Total_Amount'}, inplace=True)\n",
    "data=data.merge(aggregates, on='customer_id', how='left')\n",
    "\n",
    "\n",
    "# Extract temporal features from dates\n",
    "date_cols = ['disbursement_date', 'due_date']\n",
    "for col in date_cols:\n",
    "    data[col] = pd.to_datetime(data[col])\n",
    "    # Extract month, day, year\n",
    "    data[col+'_month'] = data[col].dt.month\n",
    "    data[col+'_day'] = data[col].dt.day\n",
    "    data[col+'_year'] = data[col].dt.year\n",
    "    # Calculate loan term and weekday features\n",
    "    data[f'loan_term_days'] = (data['due_date'] - data['disbursement_date']).dt.days\n",
    "    data[f'disbursement_weekday'] = data['disbursement_date'].dt.weekday\n",
    "    data[f'due_weekday'] = data['due_date'].dt.weekday\n",
    "\n",
    "# Create some financial ratios and transformations\n",
    "data['repayment_ratio'] = data['Total_Amount_to_Repay'] / data['Total_Amount']\n",
    "data['amount_due_per_day'] = (data['Total_Amount_to_Repay'] / data['duration'])\n",
    "data['log_Total_Amount'] = np.log1p(data['Total_Amount'])\n",
    "data['log_Total_Amount_to_Repay'] = np.log1p(data['Total_Amount_to_Repay']) \n",
    "data['log_Amount_Funded_By_Lender'] = np.log1p(data['Amount_Funded_By_Lender'])\n",
    "data['log_Lender_portion_to_be_repaid'] = np.log1p(data['Lender_portion_to_be_repaid'])\n",
    "data['amount_to_repay_greater_than_average']=data['Mean_Total_Amount'] - data['Total_Amount_to_Repay'] \n",
    "\n",
    "#some outliers were noticed in the total amount and total amount to repay fields. \n",
    "# offset this by using the 90th percentile\n",
    "q=0.9\n",
    "data['Total_Amount_to_Repay'] = np.where(data['Total_Amount_to_Repay'] >= data['Total_Amount_to_Repay'].quantile(q), data['Total_Amount_to_Repay'].quantile(q),data['Total_Amount_to_Repay'])\n",
    "data['Total_Amount'] = np.where(data['Total_Amount'] >= data['Total_Amount'].quantile(q), data['Total_Amount'].quantile(q),data['Total_Amount'])\n",
    "\n",
    "# Handle categorical variables\n",
    "cat_cols = data.select_dtypes(include='object').columns\n",
    "\n",
    "# Label encoding for other categorical columns\n",
    "le = LabelEncoder()\n",
    "for col in [col for col in cat_cols if col not in ['loan_type', 'ID']]:\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "\n",
    "# Split back into train and test\n",
    "train_df = (data[data['ID'].isin(train['ID'].unique())]).fillna(0)\n",
    "test_df = (data[data['ID'].isin(test['ID'].unique())]).fillna(0)\n",
    "\n",
    "# Define features for modeling\n",
    "features_for_modelling = [col for col in train_df.columns if col not in date_cols + ['ID', 'target', 'country_id','loan_type']]\n",
    "\n",
    "print(f\"The shape of train_df is: {train_df.shape}\")\n",
    "print(f\"The shape of test_df is: {test_df.shape}\")\n",
    "print(f\"The shape of train is: {train.shape}\")\n",
    "print(f\"The shape of test is: {test.shape}\")\n",
    "print(f\"The features for modelling are:\\n{features_for_modelling}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a815951f-5a3b-47da-9ec1-4e3366063624",
    "_uuid": "2502bd1c-e919-439f-b1ac-dace0888532c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## <a id=\"6\"></a>Model Development\n",
    "### Cross Validation\n",
    "We'll use stratified split due to class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9996cc53-2410-4594-b5f2-61d610318c3a",
    "_uuid": "c33b46fe-590a-4757-a43f-853dcb259c4a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-28T15:34:13.605417Z",
     "iopub.status.busy": "2025-03-28T15:34:13.605096Z",
     "iopub.status.idle": "2025-03-28T15:34:13.610166Z",
     "shell.execute_reply": "2025-03-28T15:34:13.608926Z",
     "shell.execute_reply.started": "2025-03-28T15:34:13.605390Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dc7befc5-750a-46e7-990d-7302e1bb311e",
    "_uuid": "e0132430-d402-4a92-a9d9-d19bd4ae358c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### Model Training and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "214734a1-540d-48d1-b1c5-7337b8283c14",
    "_uuid": "494ad22d-82e7-4ca3-a298-2b58ea194c18",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-28T15:34:13.611642Z",
     "iopub.status.busy": "2025-03-28T15:34:13.611251Z",
     "iopub.status.idle": "2025-03-28T15:34:13.636184Z",
     "shell.execute_reply": "2025-03-28T15:34:13.634963Z",
     "shell.execute_reply.started": "2025-03-28T15:34:13.611598Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "xgb_best_params= {'n_estimators': 250, 'max_depth': 7, 'learning_rate': 0.09952819042992249, 'subsample': 0.9138294343870095, 'colsample_bytree': 0.6808646076666579, 'gamma': 0.01070807358962328, 'min_child_weight': 1}\n",
    "lgb_best_params=  {'n_estimators': 250, 'max_depth': 8, 'learning_rate': 0.07587945476302646, 'num_leaves': 70, 'feature_fraction': 0.6624074561769746, 'bagging_fraction': 0.662397808134481, 'lambda_l1': 0.05808361216819946, 'lambda_l2': 0.8661761457749352}\n",
    "cat_best_params= {'iterations': 500, 'depth': 7, 'learning_rate': 0.09702561867586006}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:52:59.821112Z",
     "iopub.status.busy": "2025-03-28T15:52:59.820699Z",
     "iopub.status.idle": "2025-03-28T15:54:01.151866Z",
     "shell.execute_reply": "2025-03-28T15:54:01.150893Z",
     "shell.execute_reply.started": "2025-03-28T15:52:59.821082Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='lightgbm')\n",
    "\n",
    "\n",
    "# Coss validate model performance\n",
    "\n",
    "seed=42\n",
    "\n",
    "X,y=train_df[features_for_modelling], train_df['target']\n",
    "\n",
    "cv_reports = []\n",
    "f1_scores = []\n",
    "predictions=[]\n",
    "pred_prob=[]\n",
    "base_estimator = DecisionTreeClassifier()\n",
    "for fold, (train_idx, valid_idx) in enumerate(cv.split(X, y),1):\n",
    "    \n",
    "    X_fold_train = X.iloc[train_idx]\n",
    "    X_fold_valid = X.iloc[valid_idx]\n",
    "    y_fold_train = y.iloc[train_idx]\n",
    "    y_fold_valid = y.iloc[valid_idx]\n",
    "    \n",
    "    smote =BorderlineSMOTE(sampling_strategy=0.45, random_state=seed)\n",
    "    \n",
    "    # Apply SMOTE only on the training data of this fold\n",
    "    X_fold_train_smote, y_fold_train_smote = smote.fit_resample(X_fold_train, y_fold_train)\n",
    "    \n",
    "    # Calculate scale_pos_weight to mitigate class imbalance\n",
    "    scale_pos_weight = len(y_fold_train_smote[y_fold_train_smote == 1])*0.75 / len(y_fold_train_smote[y_fold_train_smote == 0]) \n",
    "    \n",
    "    model1 = xgb.XGBClassifier(**xgb_best_params,scale_pos_weight=scale_pos_weight,random_state=seed)\n",
    "    model2 = LGBMClassifier(**lgb_best_params,scale_pos_weight=scale_pos_weight,random_state=seed)\n",
    "    model3 = CatBoostClassifier(**cat_best_params,scale_pos_weight=scale_pos_weight,random_state=seed)\n",
    "    \n",
    "\n",
    "     \n",
    "    # Train the model on the SMOTE-balanced fold\n",
    "    model1.fit(X_fold_train_smote, y_fold_train_smote)\n",
    "    model2.fit(X_fold_train_smote, y_fold_train_smote)\n",
    "    model3.fit(X_fold_train_smote, y_fold_train_smote)\n",
    "   \n",
    "\n",
    "    # predict on validation set\n",
    "    pred_1 = model1.predict(X_fold_valid)\n",
    "    pred_2 = model2.predict(X_fold_valid)\n",
    "    pred_3 = model3.predict(X_fold_valid)\n",
    "\n",
    "    predictions = mode([pred_1,pred_2,pred_3], axis=0).mode.flatten()\n",
    "\n",
    "    report = classification_report(y_fold_valid,predictions, output_dict=True)\n",
    "    cv_reports.append(report)\n",
    "        \n",
    "        # Calculate and store the f1 score for this fold\n",
    "    f1 = f1_score(predictions, y_fold_valid)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "\n",
    "    print(f\"Fold {fold} Classification Report:\")\n",
    "    print(classification_report(y_fold_valid, predictions))\n",
    "    print(f\"Fold {fold} F1 Score: {f1:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Summary of CV results\n",
    "    print(f\"Mean F1 Score across folds: {np.mean(f1_scores):.4f}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data Prediction using Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:35:14.127438Z",
     "iopub.status.busy": "2025-03-28T15:35:14.127131Z",
     "iopub.status.idle": "2025-03-28T15:36:12.940119Z",
     "shell.execute_reply": "2025-03-28T15:36:12.939149Z",
     "shell.execute_reply.started": "2025-03-28T15:35:14.127410Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X,y=train_df[features_for_modelling], train_df['target']\n",
    "\n",
    "predictions=[]\n",
    "pred_prob=[]\n",
    "base_estimator = DecisionTreeClassifier()\n",
    "for fold, (train_idx, valid_idx) in enumerate(cv.split(X, y),1):\n",
    "    \n",
    "    X_fold_train = X.iloc[train_idx]\n",
    "    X_fold_valid = X.iloc[valid_idx]\n",
    "    y_fold_train = y.iloc[train_idx]\n",
    "    y_fold_valid = y.iloc[valid_idx]\n",
    "    \n",
    "    smote =BorderlineSMOTE(sampling_strategy=0.45, random_state=seed)\n",
    "    \n",
    "    # Apply SMOTE only on the training data of this fold\n",
    "    X_fold_train_smote, y_fold_train_smote = smote.fit_resample(X_fold_train, y_fold_train)\n",
    "        # Calculate scale_pos_weight\n",
    "    scale_pos_weight = len(y_fold_train_smote[y_fold_train_smote == 1])*0.75 / len(y_fold_train_smote[y_fold_train_smote == 0]) \n",
    "    \n",
    "    model1 = xgb.XGBClassifier(**xgb_best_params,scale_pos_weight=scale_pos_weight,random_state=seed)\n",
    "    model2 = LGBMClassifier(**lgb_best_params,scale_pos_weight=scale_pos_weight,random_state=seed)\n",
    "    model3 = CatBoostClassifier(**cat_best_params,scale_pos_weight=scale_pos_weight,random_state=seed)\n",
    "    \n",
    "     \n",
    "    # Train the model on the SMOTE-balanced fold\n",
    "    model1.fit(X_fold_train_smote, y_fold_train_smote)\n",
    "    model2.fit(X_fold_train_smote, y_fold_train_smote)\n",
    "    model3.fit(X_fold_train_smote, y_fold_train_smote)\n",
    "    \n",
    "\n",
    "    # make predictions using models\n",
    "    pred_1 = model1.predict(test_df[features_for_modelling])\n",
    "    pred_2 = model2.predict(test_df[features_for_modelling])\n",
    "    pred_3 = model3.predict(test_df[features_for_modelling])\n",
    "    pred_p_1 = model1.predict_proba(test_df[features_for_modelling])\n",
    "    pred_p_2 = model2.predict_proba(test_df[features_for_modelling])\n",
    "    pred_p_3 = model3.predict_proba(test_df[features_for_modelling])\n",
    "\n",
    "\n",
    "    #Append predictions\n",
    "    predictions.append(pred_1)\n",
    "    predictions.append(pred_2)\n",
    "    predictions.append(pred_3)\n",
    "    pred_prob.append(pred_p_1)\n",
    "    pred_prob.append(pred_p_2)\n",
    "    pred_prob.append(pred_p_3)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:36:12.941683Z",
     "iopub.status.busy": "2025-03-28T15:36:12.941276Z",
     "iopub.status.idle": "2025-03-28T15:36:13.826534Z",
     "shell.execute_reply": "2025-03-28T15:36:13.825276Z",
     "shell.execute_reply.started": "2025-03-28T15:36:12.941653Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "final_predictions = mode(predictions, axis=0).mode.flatten()\n",
    "final_pred_proba=np.mean(pred_prob,axis=0)\n",
    "test_df['target'] = final_predictions\n",
    "test_df[['proba','proba2']]= final_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:36:13.828219Z",
     "iopub.status.busy": "2025-03-28T15:36:13.827790Z",
     "iopub.status.idle": "2025-03-28T15:36:13.835987Z",
     "shell.execute_reply": "2025-03-28T15:36:13.834771Z",
     "shell.execute_reply.started": "2025-03-28T15:36:13.828178Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df.target.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Process the predictions for better score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:36:13.837627Z",
     "iopub.status.busy": "2025-03-28T15:36:13.837197Z",
     "iopub.status.idle": "2025-03-28T15:36:14.000625Z",
     "shell.execute_reply": "2025-03-28T15:36:13.999409Z",
     "shell.execute_reply.started": "2025-03-28T15:36:13.837585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# concat and sort train and test data for easy post processing\n",
    "train_test=pd.concat([train_df,test_df])\n",
    "train_test=(train_test.sort_values(['customer_id','tbl_loan_id','disbursement_date','duration'])).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:36:14.001891Z",
     "iopub.status.busy": "2025-03-28T15:36:14.001454Z",
     "iopub.status.idle": "2025-03-28T15:36:14.010259Z",
     "shell.execute_reply": "2025-03-28T15:36:14.008873Z",
     "shell.execute_reply.started": "2025-03-28T15:36:14.001851Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Post Processing functions\n",
    "\n",
    "def check_loanid_in_train_data(tbl_loan_id):\n",
    "    '''\n",
    "    The function checks loans in the test data and update the target to the\n",
    "    value available in the training data. The intuition behind this is that \n",
    "    a loan can only have one decision (0 or 1) and the target in the training data \n",
    "    supercedes that of the test data\n",
    "    '''\n",
    "    # Check if tbl_loan_id exists in the train DataFrame\n",
    "    if tbl_loan_id in train_df['tbl_loan_id'].values:\n",
    "        df = train_df[train_df['tbl_loan_id'] == tbl_loan_id]\n",
    "        target=df.target.values[0]\n",
    "    else:\n",
    "        target=3  # Returns a 3 if there is no match\n",
    "    return target\n",
    "\n",
    "    \n",
    "def type_3_correction (df):\n",
    "    '''\n",
    "    The model performed poorly in the type 3 loan type in the Ghana dataset.\n",
    "    The distribution for that loan type differs to the distribution in the training data\n",
    "    e.g. The repayment ratio for this loan type is generally much lower than the training dataset\n",
    "    making the model to predict 0 for all instances (over 3000 instances).\n",
    "    From the EDAs, it is impossible that all loans in such a class to have just one target group.\n",
    "    To offset this, an assumption that loans were being disbursed to the customer untill he fails to pay was made.\n",
    "    This function executes this assumption\n",
    "    '''\n",
    "    df1=df[df['loan_type']=='Type_3']\n",
    "    customer_id=df1.customer_id.unique()\n",
    "    for customer in customer_id:\n",
    "        loan_id=df[df['customer_id']==customer].tail(1).tbl_loan_id.values[0]\n",
    "        df.loc[df['tbl_loan_id'] == loan_id, 'target'] = 1   \n",
    "    return df\n",
    "\n",
    "\n",
    "def correct_loan_ids_with_conflicting_target(df):\n",
    "    '''\n",
    "    This function ensures that loans only have one target group (0 or 1 not both)\n",
    "    even if there is more than one lender.\n",
    "    '''\n",
    "    # Find loan_ids with different target values\n",
    "    loan_ids_with_diff_targets = df.groupby('tbl_loan_id')['target'].transform('nunique') > 1\n",
    "\n",
    "    # Update target values to 1 for those loan_ids\n",
    "    df.loc[loan_ids_with_diff_targets, 'target'] = 0\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:36:14.012521Z",
     "iopub.status.busy": "2025-03-28T15:36:14.012019Z",
     "iopub.status.idle": "2025-03-28T15:36:17.569022Z",
     "shell.execute_reply": "2025-03-28T15:36:17.567886Z",
     "shell.execute_reply.started": "2025-03-28T15:36:14.012487Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Run through the DataFrame and apply the logic explained in the preceeding cell\n",
    "for i in range(len(train_test)):\n",
    "    if pd.notna(train_test.loc[i, 'proba']):\n",
    "        customer_id=train_test.loc[i, 'customer_id']\n",
    "            \n",
    "        tbl_loan_id = train_test.loc[i, 'tbl_loan_id']\n",
    "        target=check_loanid_in_train_data(tbl_loan_id)\n",
    "            \n",
    "        if target <2:\n",
    "            train_test.loc[i, 'target']=target\n",
    "\n",
    "train_test_f=type_3_correction (train_test)\n",
    "correct_loan_ids_with_conflicting_target(train_test_f)\n",
    "test_df=train_test_f[train_test_f['proba'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:36:17.570600Z",
     "iopub.status.busy": "2025-03-28T15:36:17.570199Z",
     "iopub.status.idle": "2025-03-28T15:36:17.617228Z",
     "shell.execute_reply": "2025-03-28T15:36:17.616094Z",
     "shell.execute_reply.started": "2025-03-28T15:36:17.570558Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sub =  test_df[['ID', 'target']]\n",
    "sub.head()\n",
    "\n",
    "sub.to_csv('final_submission.csv', index=False)\n",
    "sub.target.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:57:33.067426Z",
     "iopub.status.busy": "2025-03-28T15:57:33.066995Z",
     "iopub.status.idle": "2025-03-28T15:57:33.170038Z",
     "shell.execute_reply": "2025-03-28T15:57:33.168677Z",
     "shell.execute_reply.started": "2025-03-28T15:57:33.067395Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save models\n",
    "joblib.dump(model1, '/kaggle/working/model1.pkl')\n",
    "joblib.dump(model2, '/kaggle/working/model2.pkl')\n",
    "joblib.dump(model3, '/kaggle/working/model3.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"1\"></a>Credit Score Function\n",
    "To build the credit score function, the following were considered:\n",
    "- Probability of default: This is one of the models ouputs. The higher the probability, the higher the possibility of default.\n",
    "- The loan history length: This is the number of historical loans granted to the customer. The higher the number, the higher the credit score.\n",
    "- The number of defaults: This is the number of times the customer has defaulted. The higher the number, the lower the credit score.\n",
    "- The credit score is capped between 300 and 850 to comply with the mostly used standards.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:36:17.618684Z",
     "iopub.status.busy": "2025-03-28T15:36:17.618356Z",
     "iopub.status.idle": "2025-03-28T15:36:17.644947Z",
     "shell.execute_reply": "2025-03-28T15:36:17.643265Z",
     "shell.execute_reply.started": "2025-03-28T15:36:17.618644Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Group by customer_id\n",
    "# calculate the number of loans and defaults\n",
    "\n",
    "credit_score_calc = train_test_f.groupby('customer_id').agg(\n",
    "    distinct_loans=('tbl_loan_id', 'nunique'),\n",
    "    count_default=('target', 'sum')\n",
    ").reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:36:17.648767Z",
     "iopub.status.busy": "2025-03-28T15:36:17.648436Z",
     "iopub.status.idle": "2025-03-28T15:36:17.660545Z",
     "shell.execute_reply": "2025-03-28T15:36:17.659062Z",
     "shell.execute_reply.started": "2025-03-28T15:36:17.648740Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "credit_score_calc.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:36:17.662631Z",
     "iopub.status.busy": "2025-03-28T15:36:17.662275Z",
     "iopub.status.idle": "2025-03-28T15:36:17.717322Z",
     "shell.execute_reply": "2025-03-28T15:36:17.716273Z",
     "shell.execute_reply.started": "2025-03-28T15:36:17.662590Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_test_f=train_test_f.merge(credit_score_calc, on=['customer_id'])\n",
    "train_test_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:36:17.718775Z",
     "iopub.status.busy": "2025-03-28T15:36:17.718450Z",
     "iopub.status.idle": "2025-03-28T15:36:17.730159Z",
     "shell.execute_reply": "2025-03-28T15:36:17.728769Z",
     "shell.execute_reply.started": "2025-03-28T15:36:17.718737Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calculate credit scores for the test datasets\n",
    "test_df=train_test_f[train_test_f['proba'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:36:17.731659Z",
     "iopub.status.busy": "2025-03-28T15:36:17.731312Z",
     "iopub.status.idle": "2025-03-28T15:36:17.744774Z",
     "shell.execute_reply": "2025-03-28T15:36:17.743619Z",
     "shell.execute_reply.started": "2025-03-28T15:36:17.731626Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def credit_score(prob_default, default_payment_count, loan_amount, loan_history_length):\n",
    "    \"\"\"\n",
    "    Compute a credit score based on model output and other financial factors.\n",
    "    \n",
    "    :param prob_default: Probability of default (output of the model, between 0 and 1)\n",
    "    :param default_payment_count: Number of defaults in payment in the customer's history\n",
    "    :param loan_amount: Total_Amount requested\n",
    "    :param loan_history_length: Length of loans in the customer's history\n",
    "    :return: Credit score (scaled between 300 and 850)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define weights for each factor\n",
    "    # The defined weights are only suggections and can be adjusted based on available data for better credit scoring\n",
    "    \n",
    "    weight_probab_default = -400  # High penalty for high probability of default \n",
    "    weight_default_payments = -20  # Each late payment reduces score\n",
    "    weight_loan_history_length = 5  # Longer credit history increases score\n",
    "    \n",
    "        \n",
    "    # Compute raw score\n",
    "    raw_score = (\n",
    "        850 + \n",
    "        weight_probab_default * prob_default +\n",
    "        weight_default_payments * default_payment_count +\n",
    "        weight_loan_history_length * loan_history_length\n",
    "    )\n",
    "    \n",
    "    # Ensure score is within 300-850 range\n",
    "    credit_score = np.clip(raw_score, 300, 850)\n",
    "    \n",
    "    return round(credit_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:36:17.746551Z",
     "iopub.status.busy": "2025-03-28T15:36:17.746221Z",
     "iopub.status.idle": "2025-03-28T15:36:18.270752Z",
     "shell.execute_reply": "2025-03-28T15:36:18.269744Z",
     "shell.execute_reply.started": "2025-03-28T15:36:17.746523Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#An example of how to use it with the models outputs\n",
    "\n",
    "test_df['credit_score'] = test_df.apply(\n",
    "    lambda row: credit_score(row['proba2'], row['count_default'], row['Total_Amount'], row['distinct_loans']),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:36:18.272305Z",
     "iopub.status.busy": "2025-03-28T15:36:18.271895Z",
     "iopub.status.idle": "2025-03-28T15:36:18.298513Z",
     "shell.execute_reply": "2025-03-28T15:36:18.297319Z",
     "shell.execute_reply.started": "2025-03-28T15:36:18.272264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:36:18.300390Z",
     "iopub.status.busy": "2025-03-28T15:36:18.300051Z",
     "iopub.status.idle": "2025-03-28T15:36:18.308248Z",
     "shell.execute_reply": "2025-03-28T15:36:18.307003Z",
     "shell.execute_reply.started": "2025-03-28T15:36:18.300362Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df.credit_score.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"1\"></a>Credit Score Ranges\n",
    "- 300–579: Poor\n",
    "- 580–669: Fair\n",
    "- 670–739: Good\n",
    "- 740–799: Very good\n",
    "- 800–850: Excellent\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If the income of the customer is available, the function below can be used as it incorporates the income of the customer. Doing this may improve the credit score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T15:36:18.309711Z",
     "iopub.status.busy": "2025-03-28T15:36:18.309301Z",
     "iopub.status.idle": "2025-03-28T15:36:18.326343Z",
     "shell.execute_reply": "2025-03-28T15:36:18.325118Z",
     "shell.execute_reply.started": "2025-03-28T15:36:18.309670Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def credit_score(prob_default, default_payment_count, loan_amount, loan_history_length,income):\n",
    "    \"\"\"\n",
    "    Compute a credit score based on model output and other financial factors.\n",
    "    \n",
    "    :param prob_default: Probability of default (output of the model, between 0 and 1)\n",
    "    :param default_payment_count: Number of defaults in payment in the customer's history\n",
    "    :param loan_amount: Total_Amount requested\n",
    "    :param income: The income of the customer\n",
    "    :param loan_history_length: Length of loans in the customer's history\n",
    "    :return: Credit score (scaled between 300 and 850)\n",
    "    \"\"\"\n",
    "    \n",
    "     # Define weights for each factor\n",
    "    # The defined weights are only suggections and can be adjusted based on available data for better credit scoring\n",
    "    \n",
    "    weight_probab_default = -400  # High penalty for high probability of default \n",
    "    weight_default_payments = -20  # Each late payment reduces score\n",
    "    weight_loan_history_length = 5  # Longer credit history increases score\n",
    "    weight_loan_income_ratio = -100  # High loan relative to income reduces score\n",
    "    \n",
    "    \n",
    "    # Compute loan-to-income ratio\n",
    "    loan_income_ratio = loan_amount / (income + 1e-6)  # Avoid division by zero\n",
    "    \n",
    "    # Compute raw score\n",
    "    raw_score = (\n",
    "        850 + \n",
    "        weight_probab_default * prob_default +\n",
    "        weight_default_payments * default_payment_count +\n",
    "        weight_loan_income_ratio * loan_income_ratio +\n",
    "        weight_loan_history_length * loan_history_length\n",
    "    )\n",
    "    \n",
    "    # Ensure score is within 300-850 range\n",
    "    credit_score = np.clip(raw_score, 300, 850)\n",
    "    \n",
    "    return round(credit_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6335556,
     "sourceId": 10244354,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6354716,
     "sourceId": 10270784,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
